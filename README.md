# Flash Attention
This document was prepared **only** for DS-5899 Paper Presentation at Vanderbilt University.

# Motivation of Flash Attention: Modeling Longer Sequences
* **NLP**: Large context required to understand books, plays, and instruction manuals
* **Computer Vision**: Higher resolution can lead to a better and more robust insight
* **Time series, audio, video, medical imaging**: Data are intrinsicly modeled as sequences of multiple steps

![Screen Shot 2022-10-24 at 3 51 54 PM](https://user-images.githubusercontent.com/25111091/197629589-7053887a-0ca6-4637-8e3c-4ac0df09da03.png)
