# DS-5899
Paper Presentation of Flash Attention

# Motivation of Flash Attention: Modeling Longer Sequences
NLP: Large context required to understand books, plays, and instruction manuals
Computer Vision: Higher resolution can lead to a better and more robust insight
Time series, audio, video, medical imaging: Data are intrinsicly modeled as sequences of multiple steps

